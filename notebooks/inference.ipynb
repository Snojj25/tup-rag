{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModelForCausalLM, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Determine the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load embedding_model and move to appropriate device\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\").to(device)\n",
    "\n",
    "def optimize_memory():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Pooling - Take attention mask into account for correct averaging\n",
    "def _mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[\n",
    "        0\n",
    "    ]  # First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = (\n",
    "        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    ).to(device)\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(\n",
    "        input_mask_expanded.sum(1), min=1e-9\n",
    "    )\n",
    "\n",
    "\n",
    "def embedd_sequences(sequences) -> torch.Tensor:\n",
    "    # Tokenize all sequences at once with padding\n",
    "    encoded_input = tokenizer(\n",
    "        sequences, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Move input tensors to device\n",
    "    input_ids = encoded_input[\"input_ids\"].to(device)\n",
    "    attention_mask = encoded_input[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():  # disable gradient calculation for inference\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = _mean_pooling(outputs, encoded_input[\"attention_mask\"])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    # If you need the embeddings on CPU for further processing, uncomment the following line:\n",
    "    sentence_embeddings = sentence_embeddings.cpu().numpy().tolist()\n",
    "\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional setup to handle padding properly\n",
    "def setup_tokenizer_and_model():\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        torch_dtype=torch.float16,  # Use half precision to save memory\n",
    "        device_map=\"auto\",  # Automatically handle device placement\n",
    "    )\n",
    "\n",
    "    # Ensure pad token is set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    # Configure model padding settings\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_output(prompt: str, model, tokenizer):\n",
    "\n",
    "    # Encode with padding and attention mask\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "    # Make sure pad_token is set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    # Move input tensors to GPU\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=512,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    # Move output back to CPU for decoding\n",
    "    outputs = outputs.cpu()\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res1:  What's a polar bear? (part 2)\n",
      "This is the second part of our exploration of what a polar bear is. In part 1, we learned that polar bears are the largest land carnivores, and that their fur is actually transparent, not white. In this part, we'll delve deeper into the biology and behavior of these amazing animals.\n",
      "\n",
      "**Habitat and Diet**\n",
      "Polar bears live in the Arctic Circle, specifically in the Arctic Ocean and surrounding land masses. They are well adapted to their environment, with a thick layer of fat and a white coat that helps them blend in with their snowy surroundings. Their main source of food is seals, which provide them with the energy and nutrients they need to survive. They hunt seals by waiting at the edge of breathing holes in the ice, then ambushing them when they come up to breathe.\n",
      "\n",
      "**Hibernation**\n",
      "Unlike some other bears, polar bears do not truly hibernate. Instead, they enter a state of \"walking hibernation\" or \"torpor\", where their heart rate and metabolism slow down, but they can still wake up quickly if needed. This adaptation helps them conserve energy during times of food scarcity.\n",
      "\n",
      "**Social Behavior**\n",
      "Polar bears are generally solitary animals, only coming together to mate or to protect their territory from other bears. Female polar bears have a highly developed sense of smell, which helps them detect the scent of their cubs, even after several months of separation.\n",
      "\n",
      "**Conservation Status**\n",
      "Unfortunately, polar bears are facing significant threats due to climate change. The melting of sea ice, which they rely on for hunting and breeding, is causing them to lose their primary source of food and habitat. As a result, polar bears are listed as a vulnerable species by the International Union for Conservation of Nature (IUCN).\n",
      "\n",
      "**Interesting Facts**\n",
      "* Polar bears can swim for hours or even days at a time, covering distances of up to 60 miles (97 kilometers).\n",
      "* They have black skin under their white fur, which helps them absorb heat.\n",
      "* Polar bears can slow down their heart rate to just 8 beats per minute, allowing them to conserve energy.\n",
      "* In the wild, polar bears can live up to 25-30 years, although the average lifespan is around 15-20 years.\n",
      "\n",
      "We hope you've enjoyed this second part of our exploration of what a polar bear is. Stay tuned for more fascinating facts and insights about these incredible animals!\n"
     ]
    }
   ],
   "source": [
    "optimize_memory()\n",
    "\n",
    "sequence = \"What's a polar bear?\"\n",
    "tokenizer, model = setup_tokenizer_and_model()\n",
    "\n",
    "try:\n",
    "    response = get_model_output(sequence, model=model, tokenizer=tokenizer)\n",
    "\n",
    "    print(\"res1: \", response)\n",
    "\n",
    "    # response = get_model_output_with_rag(\n",
    "    #     \"Kaj je severni medved?\",\n",
    "    #     \"Severni medved je velika zver, ki živi na severu in se prehranjuje s tjulni.\",\n",
    "    # )\n",
    "\n",
    "    # print(\"\\nres2: \", response)\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e):\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"GPU out of memory, try again with cleared cache.\")\n",
    "    else:\n",
    "        raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
