{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snojj/Desktop/FAKS/tup/rag/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "import numpy\n",
    "import torch\n",
    "\n",
    "from inference import (\n",
    "    optimize_memory,\n",
    "    setup_tokenizer_and_model,\n",
    "    get_model_output,\n",
    ")\n",
    "from utils.chunkify_docx import DocxChunker\n",
    "from utils.db_types import VectorDBType\n",
    "from utils.embedder import embedd_sequences\n",
    "\n",
    "from vector_db.milvus_connection import SimpleVectorDB\n",
    "from vector_db.chroma_connection import insert_documents_chroma, get_documents_chroma, get_collection\n",
    "\n",
    "QueryResult = Dict[str, List[str] | List[float]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify_document(\n",
    "    docx_path: str, chunk_size: int = 200, chunk_overlap: int = 40\n",
    ") -> List[str]:\n",
    "    chunker = DocxChunker(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunks = chunker.process_docx(docx_path)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def _embed_chunks(chunks: List[str]) -> numpy.ndarray:\n",
    "    embeddings = embedd_sequences(chunks)\n",
    "\n",
    "    return embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _embed_chunks(chunks: List[str]) -> numpy.ndarray:\n",
    "    embeddings = embedd_sequences(chunks)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def store_embedding_vectors_in_vector_db(\n",
    "    chunks: List[str], db_type: VectorDBType\n",
    ") -> None:\n",
    "\n",
    "    if db_type == VectorDBType.MILVUS:\n",
    "        # Store vectors in Milvus\n",
    "\n",
    "        # Initialize the vector database\n",
    "        db = SimpleVectorDB()\n",
    "\n",
    "        embedding_vectors = _embed_chunks(chunks)\n",
    "\n",
    "        # Insert vectors and metadata\n",
    "        db.insert(embedding_vectors, chunks)\n",
    "        print(f\"Inserted {len(chunks)} vectors\")\n",
    "\n",
    "        db.close()\n",
    "\n",
    "    elif db_type == VectorDBType.CHROMA:\n",
    "        # Store vectors in Chroma\n",
    "\n",
    "        collection = get_collection(\"test_rag\")\n",
    "\n",
    "        insert_documents_chroma(collection, documents=chunks)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported vector DB type: {db_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _handle_chroma_query_results(results) -> QueryResult:\n",
    "    ids, distances, documents = (\n",
    "        results[\"ids\"],\n",
    "        results[\"distances\"],\n",
    "        results[\"documents\"],\n",
    "    )\n",
    "\n",
    "    return {\"ids\": ids, \"distances\": distances, \"documents\": documents}\n",
    "\n",
    "\n",
    "def query_vector_db(query: str, db_type: VectorDBType, top_k: int = 3) -> QueryResult:\n",
    "\n",
    "    if db_type == VectorDBType.MILVUS:\n",
    "        # Store vectors in Milvus\n",
    "\n",
    "        # Initialize the vector database\n",
    "        db = SimpleVectorDB()\n",
    "\n",
    "        query_embedding = _embed_chunks([query])[0]\n",
    "\n",
    "        # Perform a search\n",
    "        results = db.search(query_embedding, top_k=top_k)\n",
    "\n",
    "        # Close connection\n",
    "        db.close()\n",
    "\n",
    "        return results\n",
    "\n",
    "    elif db_type == VectorDBType.CHROMA:\n",
    "        # Store vectors in Chroma\n",
    "        \n",
    "        collection = get_collection(\"test_rag\")\n",
    "\n",
    "        results = get_documents_chroma(\n",
    "            collection, query_texts=[query], top_k=top_k\n",
    "        )\n",
    "\n",
    "        return _handle_chroma_query_results(results)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported vector DB type: {db_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_collection(db_type: VectorDBType) -> QueryResult:\n",
    "\n",
    "    if db_type == VectorDBType.MILVUS:\n",
    "        # Store vectors in Milvus\n",
    "\n",
    "        # Initialize the vector database\n",
    "        db = SimpleVectorDB()\n",
    "\n",
    "        db.delete()\n",
    "\n",
    "        # Close connection\n",
    "        db.close()\n",
    "\n",
    "\n",
    "    elif db_type == VectorDBType.CHROMA:\n",
    "        # Store vectors in Chroma\n",
    "        \n",
    "        collection = get_collection(\"test_rag\")\n",
    "        \n",
    "        \n",
    "        # delete_collection_chroma(\"test_rag\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported vector DB type: {db_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_response(response: str, original_prompt: str) -> str:\n",
    "    # if \"[end]\" in response:\n",
    "    #     return response.split(\"[end]\", 1)[1].strip()\n",
    "\n",
    "    if response.startswith(original_prompt):\n",
    "        return response[len(original_prompt) :].strip().lstrip(\".\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(question: str, context: str = None) -> str:\n",
    "\n",
    "    prompt: str\n",
    "    if context:\n",
    "        prompt = f\"\"\"This is a segment from the Slovenian costitution: \"{context}\". Using this information can you answer the following question in one sentence: \"{question}\".\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"Anwser the following question in one sentence: \"{question}\".\"\"\"\n",
    "\n",
    "    optimize_memory()\n",
    "\n",
    "    tokenizer, model = setup_tokenizer_and_model()\n",
    "\n",
    "    try:\n",
    "        response = get_model_output(prompt=prompt, model=model, tokenizer=tokenizer)\n",
    "\n",
    "        response = clean_response(response, original_prompt=prompt)\n",
    "\n",
    "        return response\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"GPU out of memory, try clearing the cache...\")\n",
    "            raise e\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7ebe35119010>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/snojj/Desktop/FAKS/tup/rag/venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.50s/it]\n"
     ]
    }
   ],
   "source": [
    "user_query = (\n",
    "    \"What are the rights to asylum in Slovenia?\"\n",
    ")\n",
    "response = run_inference(user_query)\n",
    "print(\"Response without rag: \", response)\n",
    "\n",
    "# Example usage\n",
    "docx_path = \"../media/Constitution.docx\"\n",
    "chunks = chunkify_document(docx_path)\n",
    "\n",
    "\n",
    "store_embedding_vectors_in_vector_db(chunks, db_type=VectorDBType.CHROMA)\n",
    "\n",
    "results = query_vector_db(user_query, db_type=VectorDBType.CHROMA, top_k=2)\n",
    "\n",
    "response = run_inference(user_query, context=results[\"documents\"][0])\n",
    "print(\"Response with rag: \", response)\n",
    "\n",
    "clear_collection(db_type=VectorDBType.CHROMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
